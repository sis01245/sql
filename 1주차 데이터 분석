데이터 분석 프로세스
데이터 수집 > 전처리 > 탐색적 데이터 분석 > 예측 모델 만들기

- colab 사용 팁
  긴 설명 사용 시엔 ''' 사용
  데이터 타입 확인할 땐 type(변수명)
  
  리스트 함수
  .append(A) 는 리스트 마지막에 A를 추가할 때 사용
  .extend(리스트A)은 기존 리스트 뒷부분에 리스트A를 추가
  
  for 반복문
  for 임시변수 in 대상:
    명령어
  로 작성
  
  if 조건문
  if 조건 :
    결과 행동
  로 작성  
  
  range 함수 - range(0,5) 사용 시 [0,1,2,3,4] 출력
  
  !!!함수 만들기
  def 함수명(함수 변수1, 변수2, ...):
    수행할 코드
    return 최종 결과
  
  !!!class 만들기(틀 만들기)
  class Monster():
    hp = 100
    mp = 10
    
      def damage(self, attack):
        self.hp = self.hp-attack
    이런 식으로 틀을 만들고
    monster1 = Monster()
    monster1.damage(120)
    이런 식으로 몬스터1 만들기와 데미지 계산을 간단하게 가능
    class 내부 함수는 객체.내부함수() 식으로 사용 가능
    
  !!!Try~except 예외문 - ~를 시도해보고 에러가 나오면 예외문을 출력하도록
  try:
    명령문
  except:
    예외문
    
  !!!판다스와 데이터 프레임, 패키지
  패키지 - 다른 누군가가 이미 만들어놓은 함수, 클래스 덩어리. import로 불러옴
  판다스 - 데이터 프레임 형태의 데이터를 분석하기에 용이한 패키지
    
  import pandas as 내가 정할 이름
  이런 식으로 불러옴
  일반적으로는
  import pandas as pd 로 사용
     
  데이터 프레임은 딕셔너리 형태 items = {'A' : [~,~,~]}의 데이터를 만든 후 
  데이터프레임명 = pd.DataFrame(items) 식으로 만들 수 있음
  
  판다스 데이터프레임 함수
  - df.head() : 상위 5개 행만 출력
  - df.tail() : 하위 5개 행만 출력
  - df.sample(~) : 무작위 ~개의 행을 출력
  - pd.concat([df1, df2] : df1과 df2를 합쳐줌
  - df.to_csv('파일명.csv', index = False) : df의 데이터를 csv파일로 저장. index=False를 추가하면 데이터의 index부분은 csv에 저장 안됨
  - new_df = pd.read_table('파일명.csv', sep=',') : 파일명.csv를 new_df로 읽어들이고, sep부분은 ,로 표기

오늘 실습 과정에 대한 간략한 설명!

1. 네이버 뉴스 페이지에서, 각각의 뉴스가 가진 URL을 리스트 형태로 만들어서 저장해놓는다.
2. 각각의 뉴스가 가진 URL 에 접근하면 제목과 본문 내용이 표시가 될텐데, 그 제목과 본문 내용을 newspaper3k 패키지를 이용해서 크롤링을 진행한다.
3. 크롤링을 할 때마다 크롤링 되어온 내용을 데이터프레임에 계속 이어붙인다.
4. 크롤링이 끝났다면, 정보가 저장되어있는 데이터프레임을 csv 파일로 저장한다.

!!! 웹과 웹 스크래핑 패키지 이해하기
beautifulsoup 패키지
.select('태그명') : 태그를 입력으로 사용해서 모든 정보를 가져옴
.select('클래스명') : 클래스를 입력으로 사용
.select('#아이디') : ID를 입력으로 사용
.select('상위태그명 하위태그명') : 특정 태그를 더욱 구체적으로 지정할 때 사용
.select('상위태그명>하위태그명')
변수명.태그명.get('속성명') 이렇게 입력하면 해당 태그 내의 속성 값만 가져옴

Newspaper3k 패키지
변수명 = Article(기사url, language='ko') 으로 내용 가져옴
아래 두 함수로 기사 분석 진행
article.download()
article.parse()

위의 과정을 거치면
print(article.title) 로 제목
print(article.text) 로 본문을 불러올 수 있음


!!! 크롤링 전 웹페이지 구조 파악하기
! url 구조 파악하기(네이버 뉴스)
sid1 은 카테고리 번호
date 는 날짜
page 는 페이지
! 기사 목차 페이지에서 각 기사들의 url은 f12 누르고 기사 하나의 elements를 f12눌러서 확인한 후, url이 들어있는 태그의 위치를 확인하면 됨
네이버 기사의 경우 type06_headline 클래스의 li > dl > dt > a 태그에 url이 들어있음

